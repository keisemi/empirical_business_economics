%% LyX 2.4.1 created this file.  For more info, see https://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,japanese,nomag]{jsarticle}
\usepackage{textcomp}
\UseRawInputEncoding
\usepackage[dvipdfm,letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[active]{srcltx}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}
\onehalfspacing

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage{fullpage}
\usepackage{natbib}
\usepackage{dcolumn}

%\usepackage{doublespace}
\usepackage[dvipdfmx, bookmarkstype=toc, colorlinks=true, urlcolor=black, linkcolor=blue, citecolor=red, linktocpage=false, bookmarks=true, bookmarksnumbered=true]{hyperref}

\usepackage{pxjahyper}

\makeatother

\begin{document}
\title{第2章付録: 最尤推定法}
\date{最終更新： \today}
\maketitle
\begin{flushright}
\begin{small} 上武康亮・遠山祐太・若森直樹・渡辺安虎\\
「実証ビジネス・エコノミクス」 \\
 第2章「消費者の購買パターンを浮き彫りにする」\\
の付録\\
 \vspace{-0.2\baselineskip}
 \textcopyright 上武康亮・遠山祐太・若森直樹・渡辺安虎 \end{small} 
\par\end{flushright}

\section{最尤法の解説}

本付録においては最尤法 (Maximum Likelihood Estimation, MLE) の概略について解説を行う。最尤法は第2章で扱う離散選択モデルを含め、非線形モデルのパラメタ推定において幅広く用いられている手法である。詳細な解説については以下の教科書等を参考にされたい。
\begin{itemize}
\item 末石直也 (2015)『計量経済学：ミクロデータ分析へのいざない』日本評論社。 
\item Hayashi, F (2000) \textit{Econometrics}, Princeton University Prress. 
\item Newey, W. K. and McFadden, D. (1994) ``Large Sample Estimation and
Hypothesis Testing," \textit{Handbook of Econometrics}, Vol.4, North
Holland: 2111--2245. 
\end{itemize}

\subsection{最尤推定量の導入}

ある確率変数$Y$を考えよう。この確率変数$Y$は密度関数$f(y;\theta)$を持ち、$\theta$は未知パラメタとする。今、データとして$\{y_{i}\}_{i=1}^{n}$が存在し、各観測$y_{i}$は独立かつ同一(i.i.d.)に密度関数$f(y;\theta_{0})$から生成されているとしよう。ここで、$\theta_{0}$は真のパラメタであり、データからこのパラメタを推定することが目標である。

ここで、手元のデータ$\{y_{i}\}_{i=1}^{N}$を観察する確率をパラメタ$\theta$の関数として表現する尤度関数を考えよう。尤度関数$L(\theta)$は以下のように定義される。

\begin{align*}
L(\theta) & \equiv\prod_{i=1}^{n}f(y_{i};\theta).
\end{align*}
また、尤度関数について自然対数を取った対数尤度を以下のように定義する
\[
l(\theta)\equiv\log L(\theta)=\sum_{i=1}^{n}\log f(y_{i};\theta).
\]

最尤推定量(Maximum Likelihood Estimator)は、対数尤度を最大化するようなパラメタ$\theta$として定義される。

\[
\hat{\theta}_{n}=\operatorname{argmax}_{\theta\in\Theta}\frac{1}{n}\sum_{i=1}^{n}\log f(y_{i};\theta).
\]


\subsection{最尤推定量の例}

最尤推定量に関するいくつかの例を見ていこう。

\subsubsection{コイントス}

確率変数$Y$として、確率$p$で$1$,確率$1-p$で0をとるものを考える。すなわち、$P(Y=1)=p,P(Y=0)=1-p$とする。今、データとして、$\{y_{i}\}_{i=1}^{n}$があるとしよう。このとき、尤度関数は
\[
L(\theta)=\prod_{i=1}^{n}P(Y=y_{i})=p^{N_{1}}(1-p)^{n-N_{1}}
\]
と与えられる。ここで、$N_{0},N_{1}$はそれぞれ、$Y=0,Y=1$となるような観測の個数である。すなわち、$N_{0}=\sum_{i=1}^{n}\mathrm{1}\{y_{i}=0\},N_{1}=\sum_{i=1}^{n}\mathrm{1}\{y_{i}=1\}$である。

対数尤度関数は
\[
l(\theta)=N_{1}\log(p)+(N-N_{1})\log(1-p)
\]
となる。この対数尤度関数をパラメタ$p$について微分して一階条件を解くと、
\[
\hat{p}=\frac{N_{1}}{n}
\]
が得られる。これは、$n$回の試行の中で表が出た回数が$N_{1}$のとき、表が出る確率の推定量を、表が出た回数の比率として与えられることを意味する。

\subsubsection{正規分布の平均と分散}

確率変数$y\sim N(\mu,\sigma^{2})$を考える。未知パラメタを$\theta=(\mu,\sigma^{2})$とする。今、正規分布からi.i.d.に生成されたデータセット$\{y_{i}\}_{i=1}^{n}$があるとき、尤度関数は以下のように与えられる。
\begin{align*}
L(\theta) & =\prod_{i=1}^{n}f(y_{i};\theta)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(y_{i}-\mu)^{2}}{2\sigma^{2}}\right).
\end{align*}
また対数尤度は
\begin{align*}
\log L(\theta) & =\sum_{i=1}^{n}\left\{ -\frac{1}{2}\log2\pi\sigma^{2}-\frac{(y_{i}-\mu)^{2}}{2\sigma^{2}}\right\} \\
 & =-\frac{n}{2}\log2\pi-\frac{n}{2}\log\sigma^{2}-\sum_{i=1}^{n}\frac{\left(y_{i}-\mu\right)^{2}}{2\sigma^{2}}.
\end{align*}
対数尤度を最大化するようなパラメタについて一回条件を用いて求めよう。

\begin{align*}
\frac{\partial\log L(\theta)}{\partial\sigma^{2}} & =-\frac{n}{2}\frac{1}{\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}=0,\\
\frac{\partial\log L(\theta)}{\partial\mu} & =\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(y_{i}-\mu)=0.
\end{align*}
これらの式を解くと、最尤推定量は
\begin{align*}
\hat{\mu} & =\frac{1}{n}\sum_{i=1}^{n}y_{i},\\
\hat{\sigma}^{2} & =\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}.
\end{align*}
となる。ここで$\hat{\mu}$は標本平均に一致し、また$\hat{\sigma}^{2}$は標本分散に一致する。

\subsubsection{誤差項に正規分布を仮定した線形回帰モデル}

以下の線形回帰モデルを考えよう。
\[
y_{i}=x_{i}'\beta+u_{i},\quad u_{i}\sim N\left(0,\sigma^{2}\right).
\]
なお、$x_{i}=(x_{1i},\ldots,x_{Ki})',\beta=(\beta_{1},\ldots,\beta_{K})'$とする。ここでは誤差項に正規分布を仮定している。推定するパラメタは$\theta=\left(\beta,\sigma^{2}\right)$としてまとめられる。

ここでは説明変数$x_{i}$について条件付をした、被説明変数$y_{i}$に関する尤度関数を考えよう。

\[
L(\theta|x_{i})=\prod_{i=1}^{n}f(y_{i};\theta)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(y_{i}-x_{i}'\beta)^{2}}{2\sigma^{2}}\right).
\]
対数尤度

\begin{align*}
\log L(\theta|x_{i}) & =\sum_{i=1}^{n}\left\{ -\frac{1}{2}\log2\pi\sigma^{2}-\frac{(y_{i}-x_{i}'\beta)^{2}}{2\sigma^{2}}\right\} \\
 & =-\frac{n}{2}\log2\pi-\frac{n}{2}\log\sigma^{2}-\sum_{i=1}^{n}\frac{\left(y_{i}-x_{i}'\beta\right)^{2}}{2\sigma^{2}}.
\end{align*}
対数尤度を最大化するべく、一回条件を考えると

\begin{align*}
\frac{\partial\log L(\theta)}{\partial\sigma^{2}} & =-\frac{n}{2}\frac{1}{\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2}=0,\\
\frac{\partial\log L(\theta)}{\partial\beta} & =\frac{1}{\sigma^{2}}\sum_{i=1}^{n}x_{i}(y_{i}-x_{i}'\beta)=0.
\end{align*}
ここで回帰係数$\beta$に着目すると
\begin{align*}
\hat{\beta}^{MLE} & =\left(\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\sum_{i=1}^{n}x_{i}y_{i}\\
 & =\hat{\beta}^{OLS}.
\end{align*}
となり、最小二乗推定量と同じものが得られる。

\subsection{最尤推定量の性質}

最尤推定量は一定の仮定のもとで、一致性や漸近正規性などの漸近的性質を持つ。以下ではパラメタを$K$次元のベクトル$\theta=(\theta_{1},\ldots,\theta_{K})'\in\mathbb{R}^{K}$とする。

まず一致性として、最尤推定量はサンプルサイズが大きくなるに連れて真のパラメタ$\theta_{0}$へ確率収束する。
\[
\hat{\theta}\stackrel{p}{\longrightarrow}\theta_{0}
\]
また漸近正規性として以下の式が成立する

\[
\begin{aligned}\sqrt{n}\left(\hat{\theta}-\theta_{0}\right) & \stackrel{d}{\longrightarrow}N(0,\underbrace{V}_{(K\times K)}),\end{aligned}
\]
ここで、$(K\times K)$行列$V$は$V=H^{-1}$であり、
\begin{align*}
H & =-E\left[\left.\frac{\partial^{2}\log f(y_{i};\theta)}{\partial\theta\partial\theta'}\right|_{\theta=\theta_{0}}\right]\\
\end{align*}
として与えられる。行列$H$は期待ヘシアンと呼ばれる。なお、ヘシアンの中身は
\[
\underbrace{\frac{\partial^{2}\log f(y_{i};\theta)}{\partial\theta\partial\theta'}}_{(K\times K)}=\left(\begin{array}{ccc}
\frac{\partial^{2}\log f(y_{i};\theta)}{\partial\theta_{1}\partial\theta_{1}} & \cdots & \frac{\partial^{2}\log f(y_{i};\theta)}{\partial\theta_{1}\partial\theta_{K}}\\
\vdots & \ddots & \vdots\\
\frac{\partial^{2}\log f(y_{i};\theta)}{\partial\theta_{K}\partial\theta_{1}} & \cdots & \frac{\partial^{2}\log f(y_{i};\theta)}{\partial\theta_{K}\partial\theta_{K}}
\end{array}\right)
\]
となる。\footnote{なお、分散共分散行列$V$はしばしば、$V=H^{-1}IH^{-1}$ とも与えられる。ここで、行列$I$は$\underbrace{I}_{(K\times K)}=E\left[\left.\frac{\partial\log f(y_{i};\theta)}{\partial\theta}\right|_{\theta=\theta_{0}}\left.\frac{\partial\log f(y_{i};\theta)}{\partial\theta'}\right|_{\theta=\theta_{0}}\right]=E\left(\begin{array}{ccc}
\frac{\partial\log f(y_{i};\theta)}{\partial\theta_{1}}\frac{\partial\log f(y_{i};\theta)}{\partial\theta_{1}} & \cdots & \frac{\partial\log f(y_{i};\theta)}{\partial\theta_{1}}\frac{\partial\log f(y_{i};\theta)}{\partial\theta_{K}}\\
\vdots & \ddots & \vdots\\
\frac{\partial\log f(y_{i};\theta)}{\partial\theta_{K}}\frac{\partial\log f(y_{i};\theta)}{\partial\theta_{1}} & \cdots & \frac{\partial\log f(y_{i};\theta)}{\partial\theta_{K}}\frac{\partial\log f(y_{i};\theta)}{\partial\theta_{K}}
\end{array}\right)$である。一定の条件下(定式化の正しさ及びいくつかの正則条件)では、情報等式(information equality) $I=H^{-1}$が成立するため、$V=H^{-1}$となる。}

この結果にもとづいて、漸近分散の計算を以下のように行うことができる。まず、漸近分散の推定量として以下の$\hat{A}$を考える。
\begin{align*}
\hat{A} & =-\frac{1}{n}\sum_{i=1}^{n}\frac{\partial^{2}}{\partial\theta\partial\theta}\log f\left(y_{i},\hat{\theta}\right)\\
 & =-\frac{\partial^{2}}{\partial\theta\partial\theta}\frac{1}{N}l\left(\hat{\theta}\right)
\end{align*}
これは目的関数(をサンプルサイズで割った値)のヘシアンに相当する。なお、目的関数（対数尤度）のヘシアンは、数値計算などにおいて付随的に計算されることが多い。\footnote{第2章における離散選択モデルの推定のコードを参照されたい。}

よって漸近分散の推定量$ase(\hat{\theta})$は、
\[
ase(\hat{\theta})=\sqrt{diag(\frac{1}{n}\hat{A}^{-1})}
\]
として与えられる。ここで、$diag(X)$は行列$X$の対角要素のみを抜き出したベクトルである。
\end{document}
